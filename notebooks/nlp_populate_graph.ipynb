{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "golden-overall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import urllib\n",
    "from pprint import pprint\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "import spacy\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-imagination",
   "metadata": {},
   "source": [
    "# Configure spacy\n",
    "\n",
    "Prior to actually using spacy, we need to load in some models.  The basic model is their small core library, taken from the web: `en_core_web_sm`, which provides good, basic functionality with a small download size (< 20 MB).  However, one drawback of this basic model is that it doesn't have full word vectors.  Instead, it comes with context-sensitive tensors.  You can still do things like text similarity with it, but if you want to use spacy to create good word vectors, you should use a larger model such as `en_core_web_md` or`en_core_web_lg` since the small models are not known for accuracy.  You can also use a variety of third-party models, but that is beyond the scope of this workshop.  Again, choose the model that works best with your setup.\n",
    "\n",
    "To load the models we use the following command:\n",
    "\n",
    "`python3 -m spacy download en_core_web_sm`\n",
    "\n",
    "You can do this either as a cell in this notebook or via the CLI.\n",
    "\n",
    "## API key for Google Knowledge Graph\n",
    "\n",
    "See below for instructions on how to create this key.  When you have the key, save it to a file called `.api_key` in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "collaborative-forest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer', 'merge_noun_chunks']\n"
     ]
    }
   ],
   "source": [
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "VERBS = ['ROOT', 'advcl']\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
    "ENTITY_LABELS = ['PERSON', 'NORP', 'GPE', 'ORG', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART']\n",
    "\n",
    "api_key = open('.api_key').read()\n",
    "\n",
    "non_nc = spacy.load('en_core_web_sm')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('merge_noun_chunks')\n",
    "\n",
    "print(non_nc.pipe_names)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-munich",
   "metadata": {},
   "source": [
    "# Neo4j Connector Class\n",
    "\n",
    "There are a few different Python packages that can be used to connect to Neo4j: `neo4j` (the official driver) or `py2neo` (a community-written driver).  There are many examples out there on how to use `py2neo`, so I have chosen to use `neo4j` for the purposes of providing a different example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "applied-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jConnection:\n",
    "    \n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = pwd\n",
    "        self.__driver = None\n",
    "        try:\n",
    "            self.__driver = GraphDatabase.driver(self.__uri, auth=(self.__user, self.__pwd))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create the driver:\", e)\n",
    "        \n",
    "    def close(self):\n",
    "        if self.__driver is not None:\n",
    "            self.__driver.close()\n",
    "        \n",
    "    def query(self, query, parameters=None, db=None):\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try: \n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session() \n",
    "            response = list(session.run(query, parameters))\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally: \n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-essence",
   "metadata": {},
   "source": [
    "# Query Google Knowledge Graph\n",
    "\n",
    "To query the Google Knowledge Graph you will require an API key, which permits you to have 100,000 read calls per day per project for free.  That will be more than sufficient for this workshop.  To obtain your key, follow [these instructions](https://developers.google.com/knowledge-graph/how-tos/authorizing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "regulated-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_google(query, api_key, limit=10, indent=True, return_lists=True):\n",
    "    \n",
    "    text_ls = []\n",
    "    node_label_ls = []\n",
    "    url_ls = []\n",
    "    \n",
    "    params = {\n",
    "        'query': query,\n",
    "        'limit': limit,\n",
    "        'indent': indent,\n",
    "        'key': api_key,\n",
    "    }   \n",
    "    \n",
    "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "    url = service_url + '?' + urllib.parse.urlencode(params)\n",
    "    response = json.loads(urllib.request.urlopen(url).read())\n",
    "    \n",
    "    if return_lists:\n",
    "        for element in response['itemListElement']:\n",
    "\n",
    "            try:\n",
    "                node_label_ls.append(element['result']['@type'])\n",
    "            except:\n",
    "                node_label_ls.append('')\n",
    "\n",
    "            try:\n",
    "                text_ls.append(element['result']['detailedDescription']['articleBody'])\n",
    "                #pprint(element['result']['detailedDescription']['articleBody'])\n",
    "            except:\n",
    "                text_ls.append('')\n",
    "                \n",
    "            try:\n",
    "                url_ls.append(element['result']['detailedDescription']['url'])\n",
    "            except:\n",
    "                url_ls.append('')\n",
    "                \n",
    "        return text_ls, node_label_ls, url_ls\n",
    "    \n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-architect",
   "metadata": {},
   "source": [
    "# NLP Functions\n",
    "\n",
    "There are a variety of functions below that will use both spacy and regex to clean and prepare our data prior to loading it into the graph database.  The main goal is to create subject-verb-object (SVO) triples.  But before we can do that, we have to do some general cleaning of the data.  The below functions perform this for us in the following order:\n",
    "\n",
    "1. Use regex to remove control characters (`remove_special_characters`)\n",
    "2. Create spacy doc of (1) (`create_svo_lists`)\n",
    "3. Get list of all subjects, verbs, and objects (`create_svo_lists`)\n",
    "4. For each object, find the closest verb (`create_svo_triples`)\n",
    "5. Remove all stop words and punctuation (`remove_stop_words_and_punct`)\n",
    "6. Assemble SVO tuples (`create_svo_triples`)\n",
    "7. Remove duplicate tuples (`remove_duplicates`)\n",
    "8. Remove tuples that have dates in them (`remove_dates`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "temporal-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    \n",
    "    regex = re.compile(r'[\\n\\r\\t]')\n",
    "    clean_text = regex.sub(\" \", text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remove_stop_words_and_punct(text, print_text=False):\n",
    "    \n",
    "    result_ls = []\n",
    "    rsw_doc = non_nc(text)\n",
    "    \n",
    "    for token in rsw_doc:\n",
    "        if print_text:\n",
    "            print(token, token.is_stop)\n",
    "            print('--------------')\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            result_ls.append(str(token))\n",
    "    \n",
    "    result_str = ' '.join(result_ls)\n",
    "\n",
    "    return result_str\n",
    "\n",
    "\n",
    "def create_svo_lists(doc, print_lists=False):\n",
    "    \n",
    "    subject_ls = []\n",
    "    verb_ls = []\n",
    "    object_ls = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ in SUBJECTS:\n",
    "            #print(list(token.ancestors))\n",
    "            subject_ls.append((token.lower_, token.idx))\n",
    "        elif token.dep_ in VERBS:\n",
    "            #print('CHILDREN of ', token.text, ': ' ,list(token.children), token.idx)\n",
    "            verb_ls.append((token.lemma_, token.idx))\n",
    "        elif token.dep_ in OBJECTS:\n",
    "            #print('ANCESTORS of ', token.text, ': ', list(token.ancestors), token.idx)\n",
    "            object_ls.append((token.lower_, token.idx))\n",
    "\n",
    "    if print_lists:\n",
    "        print('SUBJECTS: ', subject_ls)\n",
    "        print('VERBS: ', verb_ls)\n",
    "        print('OBJECTS: ', object_ls)\n",
    "    \n",
    "    return subject_ls, verb_ls, object_ls\n",
    "\n",
    "\n",
    "def remove_duplicates(tup, tup_posn):\n",
    "    \n",
    "    check_val = set()\n",
    "    result = []\n",
    "    \n",
    "    for i in tup:\n",
    "        if i[tup_posn] not in check_val:\n",
    "            result.append(i)\n",
    "            check_val.add(i[tup_posn])\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_dates(tup_ls):\n",
    "    \n",
    "    clean_tup_ls = []\n",
    "    for entry in tup_ls:\n",
    "        if not entry[2].isdigit():\n",
    "            clean_tup_ls.append(entry)\n",
    "    return clean_tup_ls\n",
    "\n",
    "\n",
    "def create_svo_triples(text):\n",
    "    \n",
    "    clean_text = remove_special_characters(text)\n",
    "    doc = nlp(clean_text)\n",
    "    subject_ls, verb_ls, object_ls = create_svo_lists(doc)\n",
    "    \n",
    "    graph_tup_ls = []\n",
    "    dedup_tup_ls = []\n",
    "    clean_tup_ls = []\n",
    "    \n",
    "    for subj in subject_ls: \n",
    "        for obj in object_ls:\n",
    "            \n",
    "            dist_ls = []\n",
    "            \n",
    "            for v in verb_ls:\n",
    "                \n",
    "                # Assemble a list of distances between each object and each verb\n",
    "                dist_ls.append(abs(obj[1] - v[1]))\n",
    "                \n",
    "            # Get the index of the verb with the smallest distance to the object \n",
    "            # and return that verb\n",
    "            index_min = min(range(len(dist_ls)), key=dist_ls.__getitem__)\n",
    "            \n",
    "            # Remve stop words from subjects and object.  Note that we do this a bit\n",
    "            # later down in the process to allow for proper sentence recognition.\n",
    "\n",
    "            no_sw_subj = remove_stop_words_and_punct(subj[0])\n",
    "            no_sw_obj = remove_stop_words_and_punct(obj[0])\n",
    "            \n",
    "            # Add entries to the graph iff neither subject nor object is blank\n",
    "            if no_sw_subj and no_sw_obj:\n",
    "                tup = (no_sw_subj, verb_ls[index_min][0], no_sw_obj)\n",
    "                graph_tup_ls.append(tup)\n",
    "        \n",
    "        #clean_tup_ls = remove_dates(graph_tup_ls)\n",
    "    \n",
    "    dedup_tup_ls = remove_duplicates(graph_tup_ls, 2)\n",
    "    clean_tup_ls = remove_dates(dedup_tup_ls)\n",
    "    \n",
    "    return clean_tup_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-implement",
   "metadata": {},
   "source": [
    "# Add to DataFrame\n",
    "\n",
    "The following helper functions will be used to get the data we want into the format we need for creating the graph in the database.  They do the following:\n",
    "\n",
    "- `make_verb_edge_string`: Best practice in Neo4j is to name the edges with the format of `:Verb`.  We will just quickly create a column with a string in that format.\n",
    "- `add_columns`: For each subject column, we will do a search of the Google Knowledge Graph to obtain a description of that subject, possible node labels, and a URL with the complete information.  These will be used (if they exist, which they won't in all cases) to populate the properties in the graph.\n",
    "- `add_df_layer`: For each object in the original graph, we will then go add some more SVO triples to the graph based on those objects.  We search the Google Knowledge Graph to obtain the same information in the previous step and add it to the tuple list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "favorite-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_verb_edge_string(verb):\n",
    "    \n",
    "    return '[:' + str(verb).upper() + ']'\n",
    "\n",
    "\n",
    "def add_columns(row, limit=1, indent=True):\n",
    "    \n",
    "    params = {\n",
    "        'query': row[2],\n",
    "        'limit': limit,\n",
    "        'indent': indent,\n",
    "        'key': api_key,\n",
    "    } \n",
    "    \n",
    "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "    url = service_url + '?' + urllib.parse.urlencode(params)\n",
    "    response = json.loads(urllib.request.urlopen(url).read())\n",
    "    \n",
    "    try:\n",
    "        if response['itemListElement'][0]['result']['detailedDescription']['articleBody']:\n",
    "            text = response['itemListElement'][0]['result']['detailedDescription']['articleBody']\n",
    "    except:\n",
    "        text = ' '\n",
    "        \n",
    "    try:\n",
    "        if response['itemListElement'][0]['result']['@type']:\n",
    "            node_labels = response['itemListElement'][0]['result']['@type']\n",
    "    except:\n",
    "        node_labels = ' '\n",
    "\n",
    "    try:\n",
    "        if response['itemListElement'][0]['result']['detailedDescription']['url']:\n",
    "            link = response['itemListElement'][0]['result']['detailedDescription']['url']\n",
    "    except:\n",
    "        link = ' '\n",
    "\n",
    "    row['description'] = text\n",
    "    row['node_labels'] = node_labels\n",
    "    row['url'] = link\n",
    "        \n",
    "    return row\n",
    "\n",
    "\n",
    "def add_df_layer(df):\n",
    "\n",
    "    objects = df['object'].tolist()\n",
    "    final_tup_ls = []\n",
    "\n",
    "    for obj in objects:\n",
    "\n",
    "        text_ls, node_label_ls, url_ls = query_google(obj, api_key, limit=1)\n",
    "\n",
    "        for text in text_ls:\n",
    "            tup = create_svo_triples(text)\n",
    "            dedup_tup = remove_duplicates(tup, 2)\n",
    "            if dedup_tup:\n",
    "                final_tup_ls.extend(dedup_tup)\n",
    "                \n",
    "    new_df = pd.DataFrame(final_tup_ls, columns = ['subject', 'verb', 'object'])\n",
    "    new_df['edge_string'] = new_df['verb'].map(make_verb_edge_string)\n",
    "    new_df = new_df.apply(add_columns, axis=1)\n",
    "            \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-duncan",
   "metadata": {},
   "source": [
    "# Populate the graph from the DataFrame\n",
    "\n",
    "These are the functions that will be used to take our DataFrame and populate the Neo4j database.  We should note that the `insert_data` function takes our data and processes it in batch mode.  While not necessarily required for this small graph, it is a good idea to use it for larger graphs to speed up the writes.  (See [this blog post](https://towardsdatascience.com/create-a-graph-database-in-neo4j-using-python-4172d40f89c4) for more information.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "instrumental-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(rows):\n",
    "    \n",
    "    query = '''\n",
    "    UNWIND $rows AS item\n",
    "    MERGE (s:Subject {name: item.subject})\n",
    "    MERGE (o:Object {name: item.object, description: COALESCE(item.description, 'NOT SET'), url: COALESCE(item.url, 'NOT SET')})\n",
    "    WITH s, o, item\n",
    "    CALL apoc.create.relationship(s, item.edge_string, {}, o)\n",
    "    YIELD rel\n",
    "    RETURN COUNT(s), COUNT(o), COUNT(rel)\n",
    "    '''\n",
    "    \n",
    "    return insert_data(query, rows, batch_size=10000)\n",
    "\n",
    "\n",
    "\n",
    "def insert_data(query, rows, batch_size = 10000):\n",
    "    # Function to handle the updating the Neo4j database in batch mode.\n",
    "\n",
    "    total = 0\n",
    "    batch = 0\n",
    "    start = time.time()\n",
    "    result = None\n",
    "\n",
    "    while batch * batch_size < len(rows):\n",
    "\n",
    "        res = conn.query(query, parameters={'rows': rows[batch*batch_size:(batch+1)*batch_size].to_dict('records')})\n",
    "        if res[0]:\n",
    "            print(res[0])\n",
    "        else:\n",
    "            print(res)\n",
    "        #total += res[0]['total']\n",
    "        batch += 1\n",
    "        #result = {\"total\":total, \"batches\":batch, \"time\":time.time()-start}\n",
    "        result = {'batches': batch, 'time': time.time()-start}\n",
    "        print(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-retail",
   "metadata": {},
   "source": [
    "# Now let's get to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "practical-software",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barack Hussein Obama II ( (listen) bə-RAHK hoo-SAYN oh-BAH-mə; born August 4, 1961) is an American politician and attorney who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, Obama was the first African-American president of the United States. He previously served as a U.S. senator from Illinois from 2005 to 2008 and as an Illinois state senator from 1997 to 2004.\\nObama was born in Honolulu, Hawaii. After graduating from Columbia University in 1983, he worked as a community organizer in Chicago. In 1988, he enrolled in Harvard Law School, where he was the first black person to be president of the Harvard Law Review. After graduating, he became a civil rights attorney and an academic, teaching constitutional law at the University of Chicago Law School from 1992 to 2004. Turning to elective politics, he represented the 13th district from 1997 until 2004 in the Illinois Senate, when he ran for the U.S. Senate. Obama received national attention in 2004 with his March Senate primary win, his well-received July Democratic National Convention keynote address, and his landslide November election to the Senate. In 2008, he was nominated by the Democratic Party for president a year after his presidential campaign began, and after a close primary campaign against Hillary Clinton, Obama was elected over Republican nominee John McCain and was inaugurated alongside his running mate, Joe Biden, on January 20, 2009. Nine months later, he was named the 2009 Nobel Peace Prize laureate.\\nObama signed many landmark bills into law during his first two years in office. The main reforms that were passed include the Affordable Care Act (commonly referred to as ACA or \"Obamacare\"), although without a public health insurance option, the Dodd–Frank Wall Street Reform and Consumer Protection Act, and the Don\\'t Ask, Don\\'t Tell Repeal Act of 2010. The American Recovery and Reinvestment Act of 2009 and Tax Relief, Unemployment Insurance Reauthorization, and Job Creation Act of 2010 served as economic stimuli amidst the Great Recession. After a lengthy debate over the national debt limit, he signed the Budget Control and the American Taxpayer Relief Acts. In foreign policy, he increased U.S. troop levels in Afghanistan, reduced nuclear weapons with the United States–Russia New START treaty, and ended military involvement in the Iraq War. He ordered military involvement in Libya for the implementation of the UN Security Council Resolution 1973, contributing to the overthrow of Muammar Gaddafi. He also ordered the military operations that resulted in the deaths of Osama bin Laden and suspected Yemeni Al-Qaeda operative Anwar al-Awlaki.\\nAfter winning re-election by defeating Republican opponent Mitt Romney, Obama was sworn in for a second term in 2013. During this term, he promoted inclusion for LGBT Americans. His administration filed briefs that urged the Supreme Court to strike down same-sex marriage bans as unconstitutional (United States v. Windsor and Obergefell v. Hodges); same-sex marriage was legalized nationwide in 2015 after the Court ruled so in Obergefell. He advocated for gun control in response to the Sandy Hook Elementary School shooting, indicating support for a ban on assault weapons, and issued wide-ranging executive actions concerning global warming and immigration. In foreign policy, he ordered military intervention in Iraq in response to gains made by ISIL after the 2011 withdrawal from Iraq, continued the process of ending U.S. combat operations in Afghanistan in 2016, promoted discussions that led to the 2015 Paris Agreement on global climate change, initiated sanctions against Russia following the invasion in Ukraine and again after interference in the 2016 U.S. elections, brokered the JCPOA nuclear deal with Iran, and normalized U.S. relations with Cuba. Obama nominated three justices to the Supreme Court: Sonia Sotomayor and Elena Kagan were confirmed as justices, while Merrick Garland faced partisan obstruction from the Republican-led Senate led by Mitch McConnell, which never held hearings or a vote on the nomination. Obama left office in January 2017 and continues to reside in Washington, D.C.During Obama\\'s term in office, the United States\\' reputation abroad, as well as the American economy, significantly improved. Obama\\'s presidency has generally been regarded favorably, and evaluations of his presidency among historians, political scientists, and the general public frequently place him among the upper tier of American presidents.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = wikipedia.summary('barack obama')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "metallic-economics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46 s, sys: 52 ms, total: 46.1 s\n",
      "Wall time: 46.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('oh bah mə', 'be', 'american politician'),\n",
       " ('oh bah mə', 'be', '44th president'),\n",
       " ('oh bah mə', 'be', 'united states'),\n",
       " ('oh bah mə', 'be', 'democratic party'),\n",
       " ('oh bah mə', 'be', 'african american president')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "final_tup_ls = create_svo_triples(text)  \n",
    "final_tup_ls[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-shape",
   "metadata": {},
   "source": [
    "# Now we create the DataFrame that will be used to populate the graph...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "incoming-pocket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.05 s, sys: 76.1 ms, total: 2.13 s\n",
      "Wall time: 14.2 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "      <th>edge_string</th>\n",
       "      <th>description</th>\n",
       "      <th>node_labels</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh bah mə</td>\n",
       "      <td>be</td>\n",
       "      <td>american politician</td>\n",
       "      <td>[:BE]</td>\n",
       "      <td></td>\n",
       "      <td>[Thing]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oh bah mə</td>\n",
       "      <td>be</td>\n",
       "      <td>44th president</td>\n",
       "      <td>[:BE]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oh bah mə</td>\n",
       "      <td>be</td>\n",
       "      <td>united states</td>\n",
       "      <td>[:BE]</td>\n",
       "      <td>The United States of America, commonly known a...</td>\n",
       "      <td>[Thing, Country, Place, AdministrativeArea]</td>\n",
       "      <td>https://en.wikipedia.org/wiki/United_States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oh bah mə</td>\n",
       "      <td>be</td>\n",
       "      <td>democratic party</td>\n",
       "      <td>[:BE]</td>\n",
       "      <td>The Democratic Party is one of the two major c...</td>\n",
       "      <td>[Organization, Thing]</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Democratic_Party...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oh bah mə</td>\n",
       "      <td>be</td>\n",
       "      <td>african american president</td>\n",
       "      <td>[:BE]</td>\n",
       "      <td>The National Museum of African American Histor...</td>\n",
       "      <td>[Place, Museum, Thing, TouristAttraction, Civi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/National_Museum_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subject verb                      object edge_string  \\\n",
       "0  oh bah mə   be         american politician       [:BE]   \n",
       "1  oh bah mə   be              44th president       [:BE]   \n",
       "2  oh bah mə   be               united states       [:BE]   \n",
       "3  oh bah mə   be            democratic party       [:BE]   \n",
       "4  oh bah mə   be  african american president       [:BE]   \n",
       "\n",
       "                                         description  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2  The United States of America, commonly known a...   \n",
       "3  The Democratic Party is one of the two major c...   \n",
       "4  The National Museum of African American Histor...   \n",
       "\n",
       "                                         node_labels  \\\n",
       "0                                            [Thing]   \n",
       "1                                                      \n",
       "2        [Thing, Country, Place, AdministrativeArea]   \n",
       "3                              [Organization, Thing]   \n",
       "4  [Place, Museum, Thing, TouristAttraction, Civi...   \n",
       "\n",
       "                                                 url  \n",
       "0                                                     \n",
       "1                                                     \n",
       "2        https://en.wikipedia.org/wiki/United_States  \n",
       "3  https://en.wikipedia.org/wiki/Democratic_Party...  \n",
       "4  https://en.wikipedia.org/wiki/National_Museum_...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.DataFrame(final_tup_ls, columns = ['subject', 'verb', 'object'])\n",
    "df['edge_string'] = df['verb'].map(make_verb_edge_string)\n",
    "df = df.apply(add_columns, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-clause",
   "metadata": {},
   "source": [
    "# Connecting to Neo4j\n",
    "\n",
    "(Note that here we are using the internal networking set up by Docker.)\n",
    "\n",
    "To avoid duplication of nodes, we begin by creating some constraints on each subject and object based on their names.  For more information on this (along with how to set an index on your nodes), check out [this blog post](https://towardsdatascience.com/create-a-graph-database-in-neo4j-using-python-4172d40f89c4).  We then add every row from the DataFrame to the graph via the `create_graph` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "arranged-bracket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Record COUNT(s)=112 COUNT(o)=112 COUNT(rel)=112>\n",
      "{'batches': 1, 'time': 0.43721604347229004}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batches': 1, 'time': 0.43721604347229004}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = Neo4jConnection(uri=\"bolt://neo4j:7687\", user=\"neo4j\", pwd=\"1234\")\n",
    "conn.query('CREATE CONSTRAINT subj_constraint IF NOT EXISTS ON (s:Subject) ASSERT s.name IS UNIQUE')\n",
    "conn.query('CREATE CONSTRAINT obj_constraint IF NOT EXISTS ON (o:Object) ASSERT o.name IS UNIQUE')\n",
    "create_graph(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-northwest",
   "metadata": {},
   "source": [
    "# Build out the graph more\n",
    "\n",
    "We have what amounts to a very small graph.  So let's build that out a bit more.  We are now going to take everything from the `object` column in the initial DataFrame, query Google for it, and get the SVO triples for each of those.  This will take a minute or two, but could be parallelized or run through something like `dask` to speed it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "entire-things",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.3 s, sys: 701 ms, total: 22.1 s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_df = add_df_layer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "finished-jesus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 7)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "strange-appendix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Record COUNT(s)=506 COUNT(o)=506 COUNT(rel)=506>\n",
      "{'batches': 1, 'time': 0.26596856117248535}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batches': 1, 'time': 0.26596856117248535}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_graph(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-transcription",
   "metadata": {},
   "source": [
    "# But Michelle Obama is not in this graph???  Let's go ahead and add her..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "conditional-syndicate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('michelle lavaughn robinson obama', 'be', 'american attorney'),\n",
       " ('michelle lavaughn robinson obama', 'be', 'lady'),\n",
       " ('michelle lavaughn robinson obama', 'be', 'united states'),\n",
       " ('michelle lavaughn robinson obama', 'be', 'african american woman'),\n",
       " ('michelle lavaughn robinson obama', 'be', 'position')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "michelle = wikipedia.summary('michelle obama')\n",
    "michelle_tup_ls = create_svo_triples(michelle)\n",
    "michelle_tup_ls[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "biological-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "      <th>edge_string</th>\n",
       "      <th>description</th>\n",
       "      <th>node_labels</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>michelle lavaughn robinson obama</td>\n",
       "      <td>be</td>\n",
       "      <td>american attorney</td>\n",
       "      <td>[:BE]</td>\n",
       "      <td></td>\n",
       "      <td>[Thing]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>michelle lavaughn robinson obama</td>\n",
       "      <td>be</td>\n",
       "      <td>lady</td>\n",
       "      <td>[:BE]</td>\n",
       "      <td>The word lady is a term of respect for a girl ...</td>\n",
       "      <td>[Thing]</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Lady</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>michelle lavaughn robinson obama</td>\n",
       "      <td>be</td>\n",
       "      <td>united states</td>\n",
       "      <td>[:BE]</td>\n",
       "      <td>The United States of America, commonly known a...</td>\n",
       "      <td>[Place, Country, AdministrativeArea, Thing]</td>\n",
       "      <td>https://en.wikipedia.org/wiki/United_States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>michelle lavaughn robinson obama</td>\n",
       "      <td>be</td>\n",
       "      <td>african american woman</td>\n",
       "      <td>[:BE]</td>\n",
       "      <td></td>\n",
       "      <td>[Thing]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>michelle lavaughn robinson obama</td>\n",
       "      <td>be</td>\n",
       "      <td>position</td>\n",
       "      <td>[:BE]</td>\n",
       "      <td>A job, employment, work or occupation, is a pe...</td>\n",
       "      <td>[Thing]</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Job</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            subject verb                  object edge_string  \\\n",
       "0  michelle lavaughn robinson obama   be       american attorney       [:BE]   \n",
       "1  michelle lavaughn robinson obama   be                    lady       [:BE]   \n",
       "2  michelle lavaughn robinson obama   be           united states       [:BE]   \n",
       "3  michelle lavaughn robinson obama   be  african american woman       [:BE]   \n",
       "4  michelle lavaughn robinson obama   be                position       [:BE]   \n",
       "\n",
       "                                         description  \\\n",
       "0                                                      \n",
       "1  The word lady is a term of respect for a girl ...   \n",
       "2  The United States of America, commonly known a...   \n",
       "3                                                      \n",
       "4  A job, employment, work or occupation, is a pe...   \n",
       "\n",
       "                                   node_labels  \\\n",
       "0                                      [Thing]   \n",
       "1                                      [Thing]   \n",
       "2  [Place, Country, AdministrativeArea, Thing]   \n",
       "3                                      [Thing]   \n",
       "4                                      [Thing]   \n",
       "\n",
       "                                           url  \n",
       "0                                               \n",
       "1           https://en.wikipedia.org/wiki/Lady  \n",
       "2  https://en.wikipedia.org/wiki/United_States  \n",
       "3                                               \n",
       "4            https://en.wikipedia.org/wiki/Job  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "michelle_df = pd.DataFrame(michelle_tup_ls, columns = ['subject', 'verb', 'object'])\n",
    "michelle_df['edge_string'] = michelle_df['verb'].map(make_verb_edge_string)\n",
    "michelle_df = michelle_df.apply(add_columns, axis=1)\n",
    "michelle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "viral-tenant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Record COUNT(s)=38 COUNT(o)=38 COUNT(rel)=38>\n",
      "{'batches': 1, 'time': 0.08252167701721191}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batches': 1, 'time': 0.08252167701721191}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_graph(michelle_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "tested-blame",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Record COUNT(s)=146 COUNT(o)=146 COUNT(rel)=146>\n",
      "{'batches': 1, 'time': 0.05074310302734375}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batches': 1, 'time': 0.05074310302734375}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_michelle_df = add_df_layer(michelle_df)\n",
    "create_graph(new_michelle_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-associate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-bottom",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
